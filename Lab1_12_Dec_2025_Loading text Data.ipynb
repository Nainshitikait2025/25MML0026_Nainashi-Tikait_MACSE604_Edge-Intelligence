{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a154ff8-e16f-487b-a8ed-235841477c26",
   "metadata": {},
   "source": [
    "# Text Data Project â€“ NLP Pipeline using Project Gutenberg\n",
    "\n",
    "This notebook loads an HTML dataset from Project Gutenberg, extracts text using BeautifulSoup, and performs basic NLP tasks such as tokenization and POS tagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bb37e-3854-4530-a087-3cbde09714a2",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset (HTML File)\n",
    "\n",
    "- Dataset Source: Project Gutenberg  \n",
    "- Book Used: *Romeo and Juliet*  \n",
    "- File Name: `romeo_and_juliet.html`  \n",
    "- This section reads the HTML file and extracts the raw text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470b5640-62ee-4f30-905d-ef9b29068498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = r\"C:\\Users\\tikai\\Downloads\\Romeo_Juliet\\Romeo_Juliet.html\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "text = soup.get_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db09bd-5554-4789-8f4d-b388c6ef8b35",
   "metadata": {},
   "source": [
    "## 2. Extract Raw Text from HTML (Scraping)\n",
    "\n",
    "Using BeautifulSoup to parse the HTML content and extract clean text from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df57f6d5-173b-4987-a8c0-743c850df554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text sample:\n",
      "\n",
      "\n",
      "Romeo and Juliet | Project Gutenberg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Project Gutenberg eBook of Romeo and Juliet\n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the \n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "text = soup.get_text()\n",
    "\n",
    "print(\"Extracted text sample:\")\n",
    "print(text[:500])   # show first 500 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a17877-93ae-4ac8-bbfd-d2f9df9815d2",
   "metadata": {},
   "source": [
    "## 2. Extracting Raw Text from HTML (Scraping)\n",
    "\n",
    "Here we parse the HTML using BeautifulSoup and extract the raw text.\n",
    "This removes tags and keeps only clean text that we can use for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aefc1981-0ded-4fea-aeea-ec56f0149264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text sample:\n",
      "\n",
      "\n",
      "Romeo and Juliet | Project Gutenberg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Project Gutenberg eBook of Romeo and Juliet\n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the \n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "text = soup.get_text()\n",
    "\n",
    "print(\"Extracted text sample:\")\n",
    "print(text[:500])   # show first 500 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb44a6-577d-4868-9bfb-a33d69d65bcb",
   "metadata": {},
   "source": [
    "### 4.1 Download Required NLTK Resources\n",
    "\n",
    "We download the tokenizers and POS tagger models needed for NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb712686-9bca-44b7-93d2-8f567f68e692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tikai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tikai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c69a23-c72f-4e9f-8ddf-dd98cac9dedc",
   "metadata": {},
   "source": [
    "### 4.2 Sentence Tokenization\n",
    "\n",
    "Splitting the extracted text into meaningful sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbc374c-3d78-4e96-a11d-a58ddcf4120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 3285\n",
      "['\\n\\nRomeo and Juliet | Project Gutenberg\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Project Gutenberg eBook of Romeo and Juliet\\nThis ebook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever.', 'You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this ebook or online\\nat www.gutenberg.org.', 'If you are not located in the United States,\\nyou will have to check the laws of the country where you are located\\nbefore using this eBook.', 'Title: Romeo and Juliet\\n\\nAuthor: William Shakespeare\\n\\nRelease date: November 1, 1998 [eBook #1513]\\n                Most recently updated: September 18, 2025\\nLanguage: English\\nCredits: the PG Shakespeare Team, a team of about twenty Project Gutenberg volunteers\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK ROMEO AND JULIET ***\\n\\nTHE TRAGEDY OF ROMEO AND JULIET\\nby William Shakespeare\\n\\n\\nContents\\n\\n\\n THE PROLOGUE.', 'ACT I\\n\\n\\n Scene I.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Total sentences:\", len(sentences))\n",
    "print(sentences[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85d861-6f7e-42a3-ba21-726a47eac12c",
   "metadata": {},
   "source": [
    "### 4.3 Word Tokenization\n",
    "\n",
    "Breaking the text into individual words (tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394a9f14-07d6-48b1-941a-2dcde4610bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 37615\n",
      "['Romeo', 'and', 'Juliet', '|', 'Project', 'Gutenberg', 'The', 'Project', 'Gutenberg', 'eBook', 'of', 'Romeo', 'and', 'Juliet', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(\"Total words:\", len(words))\n",
    "print(words[:40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0ec24-8cb6-4f9e-afa1-1a6a1bac23c6",
   "metadata": {},
   "source": [
    "### 4.4 POS (Part-of-Speech) Tagging\n",
    "\n",
    "Assigning grammatical tags (noun, verb, adjective, etc.) to each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd255b97-4f34-4598-9f10-b338b8c308ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags for first 50 words:\n",
      "[('Romeo', 'NNP'), ('and', 'CC'), ('Juliet', 'NNP'), ('|', 'NNP'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('The', 'DT'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('eBook', 'NN'), ('of', 'IN'), ('Romeo', 'NNP'), ('and', 'CC'), ('Juliet', 'NNP'), ('This', 'DT'), ('ebook', 'NN'), ('is', 'VBZ'), ('for', 'IN'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('anyone', 'NN'), ('anywhere', 'RB'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('and', 'CC'), ('most', 'JJS'), ('other', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('world', 'NN'), ('at', 'IN'), ('no', 'DT'), ('cost', 'NN'), ('and', 'CC'), ('with', 'IN'), ('almost', 'RB'), ('no', 'DT'), ('restrictions', 'NNS'), ('whatsoever', 'RB'), ('.', '.'), ('You', 'PRP'), ('may', 'MD'), ('copy', 'VB'), ('it', 'PRP'), (',', ','), ('give', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_tags = pos_tag(words[:50])  # tag a sample\n",
    "print(\"POS Tags for first 50 words:\")\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6716489-f541-4072-ad0e-c47baae9a5b8",
   "metadata": {},
   "source": [
    "## 5. Results Summary\n",
    "\n",
    "- Sentence tokenization completed  \n",
    "- Word tokenization completed  \n",
    "- POS tagging completed  \n",
    "- Output samples are displayed in each step  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd787bc3-e5ac-404d-abe1-253bd6ccf21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "- I downloaded an HTML dataset (Romeo & Juliet) from Project Gutenberg.\n",
    "- Loaded and scraped the HTML using BeautifulSoup.\n",
    "- Extracted raw text and created a dataset sample file (`text_sample.txt`).\n",
    "- Performed sentence tokenization, word tokenization, and POS tagging using NLTK.\n",
    "- Prepared the notebook with proper sections and documentation for submission.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
